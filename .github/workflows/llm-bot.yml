name: LLM Bot - Legacy (Manual Only)

on:
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number to respond to'
        required: true
        default: '24'
      comment_body:
        description: 'Comment text (/ask or /plan command)'
        required: true

# Required for OIDC to GCP
permissions:
  id-token: write
  issues: write
  contents: read

jobs:
  llm-response:
    # Only run on Issue #24 (Control Room)
    if: github.event.issue.number == 24
    runs-on: ubuntu-latest
    
    steps:
      - name: Command Guard - Only /ask or /plan
        id: command_check
        run: |
          BODY=$(cat << 'EOF'
          ${{ github.event.comment.body }}
          EOF
          )
          
          # Check if comment starts with /ask or /plan
          if echo "$BODY" | grep -qE '^\s*/ask|^\s*/plan'; then
            echo "skip=false" >> $GITHUB_OUTPUT
            # Extract command and query
            COMMAND=$(echo "$BODY" | grep -oE '^\s*/[a-z]+' | tr -d ' ')
            QUERY=$(echo "$BODY" | sed -E 's/^\s*\/[a-z]+\s*//')
            echo "command=$COMMAND" >> $GITHUB_OUTPUT
            echo "query=$QUERY" >> $GITHUB_OUTPUT
            echo "::notice::Command detected: $COMMAND"
          else
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "::notice::Skipping: No /ask or /plan command"
          fi
      
      - name: Bot Guard - Skip if comment from Bot user
        id: bot_check
        if: steps.command_check.outputs.skip == 'false'
        run: |
          if [ "${{ github.event.comment.user.type }}" == "Bot" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "::notice::Skipping: Bot user detected (${{ github.event.comment.user.login }})"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Checkout repository
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        uses: actions/checkout@v4
      
      - name: Authenticate to GCP via OIDC
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        id: auth
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/673161610630/locations/global/workloadIdentityPools/github-actions-pool/providers/github-provider'
          service_account: 'github-actions-deployer@project-38-ai.iam.gserviceaccount.com'
      
      - name: Get LLM API Keys from Secret Manager
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        id: secrets
        uses: google-github-actions/get-secretmanager-secrets@v2
        with:
          secrets: |-
            OPENAI_API_KEY:project-38-ai/openai-api-key
            ANTHROPIC_API_KEY:project-38-ai/anthropic-api-key
            GEMINI_API_KEY:project-38-ai/gemini-api-key
      
      - name: Load Context from Repository
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        id: context
        run: |
          # Load SYSTEM_MAP.md
          SYSTEM_MAP=$(cat docs/_system/SYSTEM_MAP.md)
          echo "system_map<<EOF" >> $GITHUB_OUTPUT
          echo "$SYSTEM_MAP" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Load phase_status.md
          PHASE_STATUS=$(cat docs/context/phase_status.md)
          echo "phase_status<<EOF" >> $GITHUB_OUTPUT
          echo "$PHASE_STATUS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "::notice::Context loaded: SYSTEM_MAP.md + phase_status.md"
      
      - name: Generate LLM Response
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        id: llm
        env:
          ANTHROPIC_API_KEY: ${{ steps.secrets.outputs.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ steps.secrets.outputs.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ steps.secrets.outputs.GEMINI_API_KEY }}
        run: |
          # Create prompt with context
          cat > prompt.txt << 'PROMPT_EOF'
          You are Project 38 AI assistant responding to commands in the Control Room (GitHub Issue #24).
          
          SYSTEM CONTEXT:
          ${{ steps.context.outputs.system_map }}
          
          PHASE STATUS:
          ${{ steps.context.outputs.phase_status }}
          
          USER COMMAND: ${{ steps.command_check.outputs.command }}
          USER QUERY: ${{ steps.command_check.outputs.query }}
          
          Please provide a concise, actionable response based on the current project state.
          Keep responses under 500 words. Use markdown formatting.
          PROMPT_EOF
          
          # Call Anthropic API (Claude)
          RESPONSE=$(curl -s https://api.anthropic.com/v1/messages \
            -H "Content-Type: application/json" \
            -H "x-api-key: $ANTHROPIC_API_KEY" \
            -H "anthropic-version: 2023-06-01" \
            -d @- << 'API_EOF'
          {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 1024,
            "messages": [{
              "role": "user",
              "content": "$(cat prompt.txt | tr '\n' ' ')"
            }]
          }
          API_EOF
          )
          
          # Extract response text
          LLM_TEXT=$(echo "$RESPONSE" | jq -r '.content[0].text // "Error: Unable to generate response"')
          
          # Save to output
          echo "response<<LLM_EOF" >> $GITHUB_OUTPUT
          echo "$LLM_TEXT" >> $GITHUB_OUTPUT
          echo "LLM_EOF" >> $GITHUB_OUTPUT
          
          echo "::notice::LLM response generated ($(echo "$LLM_TEXT" | wc -w) words)"
      
      - name: Post LLM Response to Issue
        if: steps.command_check.outputs.skip == 'false' && steps.bot_check.outputs.skip == 'false'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # Format response with metadata
          cat > response.md << 'RESPONSE_EOF'
          ## ðŸ¤– LLM Response (${{ steps.command_check.outputs.command }})
          
          ${{ steps.llm.outputs.response }}
          
          ---
          *Command: `${{ steps.command_check.outputs.command }}` | Model: Claude 3.5 Sonnet | Run: [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})*
          <!-- P38_LLM_RESPONSE -->
          RESPONSE_EOF
          
          # Post to issue
          gh api \
            --method POST \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            /repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/comments \
            -F body=@response.md
          
          echo "::notice::LLM response posted for comment ${{ github.event.comment.id }}"
